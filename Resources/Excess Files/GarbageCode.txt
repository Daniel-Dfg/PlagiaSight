Phase1:
    #Code1:
        from numpy import array, concatenate, isin
        from nltk.tokenize import sent_tokenize, RegexpTokenizer
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer

        class Prepare:
            def __init__(self, file:str) -> None:
                self.file = file
                self.textNoEnline: str = self.getText()
                self.sentencesArray: array = array(sent_tokenize(self.textNoEnline))
                self.NoPunctuationsArray: array = self.getNoPunctuations()
                self.NoStopWordsArray: array = self.getNoStopWords()
                self.lemmaArry: array = self.lemma()
                
                
            def getText(self) -> str:
                with open(self.file, 'r', encoding="utf-8-sig") as f:
                    text = f.read()
                    text = array(text.split("\n"))
                    textNoEndline = " ".join(text)
                    return textNoEndline.lower()
            
            def getNoPunctuations(self) -> array:
                rt = RegexpTokenizer(r'\w+')
                textNoPunctuations = array(rt.tokenize(self.textNoEnline))
                return textNoPunctuations
            
            def getNoStopWords(self) -> array:
                sw = array(stopwords.words("english"))
                mask = isin(self.NoPunctuationsArray, sw)
                return self.NoPunctuationsArray[~mask]
            
            def lemma(self) -> array:
                lemma = WordNetLemmatizer()
                lemmawords = array([])
                for word in self.NoPunctuationsArray:
                    noun = lemma.lemmatize(word, 'n')
                    verbe = lemma.lemmatize(noun, 'v')
                    adjective = lemma.lemmatize(verbe,'a')
                    adverb = lemma.lemmatize(adjective, 'r')
                    satellite = lemma.lemmatize(adverb, 's')
                    lemmawords = concatenate((lemmawords, array([satellite])))
                return lemmawords
    #Code2:
        from dataclasses import dataclass, field
        from os import path
        import nltk
        from nltk.tokenize import wordpunct_tokenize, sent_tokenize
        from nltk.corpus import stopwords, wordnet
        from nltk.stem import WordNetLemmatizer
        from nltk import FreqDist, pos_tag
        nltk.download('wordnet')

        @dataclass
        class Tokenizer:
            """
            -> Extracts and Tokenizes data from source
            --> Cleans the data to make it analyzable

            - contains the tokenization methods and the tokens themselves (using @property)
            """
            file: str
            _raw_data: str = field(init=False, repr=False)
            _tokens_by_sentence: list[str] = field(init=False, default_factory=list, repr=False)
            _tokens_by_wordpunct: list[str] = field(init=False, default_factory=list, repr=False)

            def __post_init__(self):
                self.extract_raw_from_file()
                self.tokenize_text()
                self.clean_data()

            def extract_raw_from_file(self):
                if not path.exists(self.file):
                    raise FileNotFoundError("Specified file doesn't exist")
                with open(self.file, 'r') as f:
                    self._raw_data = f.read()

            def tokenize_text(self):
                self._tokens_by_sentence = sent_tokenize(self._raw_data)
                self._tokens_by_wordpunct = wordpunct_tokenize(self._raw_data)

            def clean_data(self):
                stop_words = set(stopwords.words("english"))  # from nltk.corpus
                lemmatizer = WordNetLemmatizer()

                # removing capital letters
                self._tokens_by_sentence = [token.lower() for token in self._tokens_by_sentence]
                self._tokens_by_wordpunct = [token.lower() for token in self._tokens_by_wordpunct]

                # removing stop words
                self._tokens_by_wordpunct = [token for token in self._tokens_by_wordpunct if token not in stop_words]
                self._tokens_by_sentence = [token for token in self._tokens_by_sentence if token not in stop_words]

                # lemmatization
                self._tokens_by_wordpunct = [lemmatizer.lemmatize(token) for token in self._tokens_by_wordpunct]
                self._tokens_by_sentence = [lemmatizer.lemmatize(token) for token in self._tokens_by_sentence]

            @property
            def tokens_by_sentence(self):
                if not self._tokens_by_sentence:
                    print("Warning: empty list of tokens by sentence. \nPlease make sure that there's data to work with.")
                return self._tokens_by_sentence

            @property
            def tokens_by_wordpunct(self):
                if not self._tokens_by_wordpunct:
                    print("Warning: empty list of tokens by word/punctuation. \nPlease make sure that there's data to work with.")
                return self._tokens_by_wordpunct

            @property
            def raw_data(self):
                if not self._raw_data:
                    print("Warning: raw data is empty")
                return self._raw_data

        @dataclass
        class TextProcessingStatistics(Tokenizer):
            """
            MUST be used with ONE set of Tokens.
            - Provides statistical insights over ONE source (word frequency, text richness, POS...)
            """
            word_freq_wordpunct: FreqDist = field(init=False)
            text_richness: float = field(init=False)
            part_of_speeches: list = field(init=False)
            average_sentence_length : float = field(init=False)

            def __post_init__(self):
                super().__post_init__()
                self.calculate_statistics()

            def calculate_statistics(self):
                self.word_freq_wordpunct = FreqDist(self.tokens_by_wordpunct)
                self.text_richness = len(self.tokens_by_wordpunct) / len(set(self.tokens_by_wordpunct))
                self.part_of_speeches = pos_tag(self.tokens_by_wordpunct)

            def display_statistics(self):
                print(f"Word Frequency (WordPunct): {self.word_freq_wordpunct}")
                print(f"Text Richness: {self.text_richness}")
                print(f"Part of Speeches: {self.part_of_speeches}")
    #Code3:
        import re
        import nltk
        from numpy import array, concatenate, isin
        from nltk.tokenize import wordpunct_tokenize, sent_tokenize
        from nltk.corpus import stopwords, wordnet
        from nltk.stem import WordNetLemmatizer
        from nltk import FreqDist, pos_tag
        from dataclasses import dataclass, field
        from os import path
        from nltk.tokenize import RegexpTokenizer

        @dataclass
        class Tokenizer:
            """
            -> Extracts and Tokenizes data from source
            --> Cleans the data to make it analyzable

            - contains the tokenization methods and the tokens themselves (using @property)
            """
            file: str
            _raw_data: str = field(init=False, repr=False)
            _tokens_by_sentence: array = field(init=False, repr=False)
            _tokens_by_wordpunct: array = field(init=False,  repr=False)
            _tokens_by_word: array = field(init=False, repr=False)

            def __post_init__(self):
                self.extract_raw_from_file()
                self.tokenize_text()
                self.clean_data()

            def extract_raw_from_file(self):
                if not path.exists(self.file):
                    raise FileNotFoundError("Specified file doesn't exist")
                with open(self.file, 'r', encoding='utf-8-sig') as f:
                    self._raw_data = f.read().lower()

            def tokenize_text(self):
                self._tokens_by_sentence = array(sent_tokenize(self.raw_data))
                self._tokens_by_wordpunct = array(wordpunct_tokenize(self.raw_data))
                rt = RegexpTokenizer(r'\w+')
                self._tokens_by_word = array(rt.tokenize(self.raw_data))

            def clean_data(self):
                stop_words = set(stopwords.words("english"))  # from nltk.corpus
                lemmatizer = WordNetLemmatizer()

                # removing stop words
                self._tokens_by_wordpunct = array([token for token in self.tokens_by_wordpunct if token not in stop_words])
                self._tokens_by_sentence = array([token for token in self.tokens_by_sentence if token not in stop_words])
                self._tokens_by_word = array([token for token in self.tokens_by_word if token not in stop_words])

                # lemmatization (TO BE FIXED #TODO)
                self._tokens_by_wordpunct = array([lemmatizer.lemmatize(token) for token in self.tokens_by_wordpunct])
                self._tokens_by_sentence = array([lemmatizer.lemmatize(token) for token in self.tokens_by_sentence])
                ...

            @property
            def tokens_by_sentence(self):
                if self._tokens_by_sentence.size == 0:
                    print("Warning: empty list of tokens by sentence. \nPlease make sure that there's data to work with.")
                return self._tokens_by_sentence

            @property
            def tokens_by_wordpunct(self):
                if self._tokens_by_wordpunct.size == 0:
                    print("Warning: empty list of tokens by word/punctuation. \nPlease make sure that there's data to work with.")
                return self._tokens_by_wordpunct
            
            @property
            def tokens_by_word(self):
                if self._tokens_by_word.size == 0:
                    print("Warning: empty list of tokens by word. \nPlease make sure that there's data to work with.")
                return self._tokens_by_word

            @property
            def raw_data(self):
                if not self._raw_data:
                    print("Warning: raw data is empty")
                return self._raw_data

        @dataclass
        class TextProcessingStatistics:
            """
            MUST be used with ONE set of Tokens.
            - Provides statistical insights over ONE source (word frequency, text richness, POS...)
            """
            base_content: Tokenizer
            word_freq_wordpunct: FreqDist = field(init=False, repr=False)
            text_richness: float = field(init=False, repr=False)
            part_of_speeches: list = field(init=False, repr=False)
            average_sentence_length: float = field(init=False, repr=False)

            def __post_init__(self):
                self.calculate_statistics()

            def calculate_statistics(self):
                self.word_freq_wordpunct = FreqDist(self.base_content.tokens_by_wordpunct)
                self.text_richness = len(self.base_content.tokens_by_wordpunct) / len(set(self.base_content.tokens_by_wordpunct))
                self.part_of_speeches = pos_tag(self.base_content.tokens_by_wordpunct)

            def display_statistics(self):
                print(f"Word Frequency (WordPunct): {self.word_freq_wordpunct}")
                print(f"Text Richness: {self.text_richness}")
                print(f"Part of Speeches: {self.part_of_speeches}")

        @dataclass
        class TextProcessingAlgorithms:
            """
            #TODO
            MUST be used with TWO sets of Tokens.
            Applies fundamental algorithms to evaluate similarity between TWO sources 
            (like cosine similarity, KMP...) and stores the results.
            """
            pass
    #Code4
        import numba
        from numpy import array, concatenate, isin
        from nltk.tokenize import wordpunct_tokenize, sent_tokenize
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer
        from nltk import FreqDist, pos_tag
        from dataclasses import dataclass, field
        from os import path
        from nltk.tokenize import RegexpTokenizer
        import numpy as np

        @dataclass
        @numba.jit
        class Tokenizer:
            """
            -> Extracts and Tokenizes data from source
            --> Cleans the data to make it analyzable

            - contains the tokenization methods and the tokens themselves (using @property)
            """
            file: str
            _raw_data: str = field(init=False, repr=False)
            _tokens_by_sentence: array = field(init=False, repr=False)
            _tokens_by_wordpunct: array = field(init=False,  repr=False)
            _tokens_by_word: array = field(init=False, repr=False)

            def __post_init__(self):
                self.extract_raw_from_file()
                self.tokenize_text()
                self.clean_data()

            def extract_raw_from_file(self):
                if not path.exists(self.file):
                    raise FileNotFoundError("Specified file doesn't exist")
                with open(self.file, 'r', encoding='utf-8-sig') as f:
                    self._raw_data = f.read().lower()

            def tokenize_text(self):
                self._tokens_by_sentence = array(sent_tokenize(self.raw_data))
                self._tokens_by_wordpunct = array(wordpunct_tokenize(self.raw_data))
                rt = RegexpTokenizer(r'\w+')
                self._tokens_by_word = array(rt.tokenize(self.raw_data))

            def clean_data(self):

                # removing stop words
                self._tokens_by_wordpunct = self.remove_stopwords(self._tokens_by_wordpunct)
                self._tokens_by_sentence = self.remove_stopwords(self._tokens_by_sentence)
                self._tokens_by_word = self.remove_stopwords(self._tokens_by_word)

                # lemmatizations
                self._tokens_by_wordpunct = array([self.lemmatize_tokens(token) for token in self.tokens_by_wordpunct])
                self._tokens_by_sentence = array([self.lemmatize_tokens(token) for token in self.tokens_by_sentence])
                
            def remove_stopwords(tokens):
                    stop_words = set(stopwords.words("english"))
                    mask = np.isin(tokens, list(stop_words), invert=True)
                    return tokens[~mask]
                
            def lemmatize_tokens(tokens:str) -> str:
                lemmatizer = WordNetLemmatizer()
                tokens = lemmatizer.lemmatize(tokens, 'n')
                tokens = lemmatizer.lemmatize(tokens, 'v')
                tokens = lemmatizer.lemmatize(tokens, 'a')
                tokens = lemmatizer.lemmatize(tokens, 'r')
                tokens = lemmatizer.lemmatize(tokens, 's')
                return tokens
            
            @property
            def tokens_by_sentence(self):
                if self._tokens_by_sentence.size == 0:
                    print("Warning: empty list of tokens by sentence. \nPlease make sure that there's data to work with.")
                return self._tokens_by_sentence

            @property
            def tokens_by_wordpunct(self):
                if self._tokens_by_wordpunct.size == 0:
                    print("Warning: empty list of tokens by word/punctuation. \nPlease make sure that there's data to work with.")
                return self._tokens_by_wordpunct
            
            @property
            def tokens_by_word(self):
                if self._tokens_by_word.size == 0:
                    print("Warning: empty list of tokens by word. \nPlease make sure that there's data to work with.")
                return self._tokens_by_word

            @property
            def raw_data(self):
                if not self._raw_data:
                    print("Warning: raw data is empty")
                return self._raw_data

        @dataclass
        @numba.jit
        class TextProcessingStatistics:
            """
            MUST be used with ONE set of Tokens.
            - Provides statistical insights over ONE source (word frequency, text richness, POS...)
            """
            base_content: Tokenizer
            word_freq_wordpunct: FreqDist = field(init=False, repr=False)
            text_richness: float = field(init=False, repr=False)
            part_of_speeches: list = field(init=False, repr=False)
            average_sentence_length: float = field(init=False, repr=False)

            def __post_init__(self):
                self.calculate_statistics()

            def calculate_statistics(self):
                self.word_freq_wordpunct = FreqDist(self.base_content.tokens_by_wordpunct)
                self.text_richness = len(self.base_content.tokens_by_wordpunct) / len(set(self.base_content.tokens_by_wordpunct))
                self.part_of_speeches = pos_tag(self.base_content.tokens_by_wordpunct)

            def display_statistics(self):
                print(f"Word Frequency (WordPunct): {self.word_freq_wordpunct}")
                print(f"Text Richness: {self.text_richness}")
                print(f"Part of Speeches: {self.part_of_speeches}")

        @dataclass
        @numba.jit
        class TextProcessingAlgorithms:
            """
            #TODO
            MUST be used with TWO sets of Tokens.
            Applies fundamental algorithms to evaluate similarity between TWO sources 
            (like cosine similarity, KMP...) and stores the results.
            """
            pass
    #Code5:
    import re
    import nltk
    import numba

    from string import punctuation # helps to check if a string is made of punctuation only here
    from numpy import array, append
    from nltk.tokenize import wordpunct_tokenize
    from nltk.corpus import stopwords, wordnet
    from nltk.stem import WordNetLemmatizer
    from nltk import FreqDist, pos_tag
    from dataclasses import dataclass, field
    from os import path
    from nltk.tokenize import RegexpTokenizer



    @dataclass
    class Tokenizer:
        """
        -> Extracts and Tokenizes data from source
        --> Cleans the data to make it analyzable

        - contains the tokenization methods and the tokens themselves (using @property)
        """
        file: str
        _raw_data: str = field(init=False, repr=False)
        _tokens_by_sentence: array = field(init=False, repr=False)
        _tokens_by_wordpunct: array = field(init=False,  repr=False)
        _tokens_by_word: array = field(init=False, repr=False)

        def __post_init__(self):
            self.extract_raw_from_file()
            self.tokenize_text()
            self.clean_data()

        def extract_raw_from_file(self):
            if not path.exists(self.file):
                raise FileNotFoundError("Specified file doesn't exist")
            with open(self.file, 'r', encoding='utf-8-sig') as f:
                self._raw_data = f.read().lower()

        def tokenize_text(self):
            regexes_sentences = RegexpTokenizer(r'\n|\. ')
            self._tokens_by_sentence = array(regexes_sentences.tokenize(self.raw_data))
            self._tokens_by_wordpunct = array(wordpunct_tokenize(self.raw_data))
            self._tokens_by_word = array([])  # will be created after tokens by wordpunct curation

        def clean_data(self):
            lemmatizer = WordNetLemmatizer()

            # removing stop words
            stop_words = set(stopwords.words('english'))
            self._tokens_by_wordpunct = array([token for token in self.tokens_by_wordpunct if token not in stop_words])

            # lemmatization (TO BE FIXED #TODO)
            self.part_of_speeches = pos_tag(self.tokens_by_wordpunct)
            for i, element in enumerate(self.part_of_speeches):
                # assuming len(self.p_o_s) == len(self.tokens_by_wp)
                token, tag = element[0], element[1]
                if not self.is_only_punctuation(token):
                    self._tokens_by_wordpunct[i] = lemmatizer.lemmatize(token, self.get_wordnet_pos(tag))
                    self._tokens_by_word = append(self._tokens_by_word, self._tokens_by_wordpunct[i]) 
                else:
                    # corrects eventual mistakes (like tagging a '.' as a noun for example)
                    self.part_of_speeches[i] = (self.part_of_speeches[i][0], "Punct")
                    # note that self.p_o_s will be used when analyzing the data, that explains why it must be corrected now


        def is_only_punctuation(self, token):
            return all(char in punctuation for char in token)

        def get_wordnet_pos(self, tag):
            if tag.startswith('J'):
                return wordnet.ADJ
            elif tag.startswith('V'):
                return wordnet.VERB
            elif tag.startswith('R'):
                return wordnet.ADV
            else:
                return wordnet.NOUN # By default

        @property
        def tokens_by_sentence(self):
            if self._tokens_by_sentence.size == 0:
                print("Warning: empty list of tokens by sentence. \nPlease make sure that there's data to work with.")
            return self._tokens_by_sentence

        @property
        def tokens_by_wordpunct(self):
            if self._tokens_by_wordpunct.size == 0:
                print("Warning: empty list of tokens by word/punctuation. \nPlease make sure that there's data to work with.")
            return self._tokens_by_wordpunct

        @property
        def tokens_by_word(self):
            if self._tokens_by_word.size == 0:
                print(
                    "Warning: empty list of tokens by word. \nPlease make sure that there's data to work with.")
            return self._tokens_by_word

        @property
        def raw_data(self):
            if not self._raw_data:
                print("Warning: raw data is empty")
            return self._raw_data


    @dataclass
    class TextProcessingStatistics:
        """
        MUST be used with ONE set of Tokens.
        - Provides statistical insights over ONE source (word frequency, text richness, POS...)
        """
        base_content: Tokenizer
        word_freq_wordpunct: FreqDist = field(init=False, repr=False)
        text_richness: float = field(init=False, repr=False)
        average_sentence_length: float = field(init=False, repr=False)

        def __post_init__(self):
            self.calculate_statistics()

        def calculate_statistics(self):
            self.word_freq_wordpunct = FreqDist(self.base_content.tokens_by_wordpunct)
            self.text_richness = len(self.base_content.tokens_by_wordpunct) / \
                len(set(self.base_content.tokens_by_wordpunct))

        def display_statistics(self):
            print(f"Word Frequency (WordPunct): {self.word_freq_wordpunct}")
            print(f"Text Richness: {self.text_richness}")


    @dataclass
    class TextProcessingAlgorithms:
        """
        #TODO
        MUST be used with TWO sets of Tokens.
        Applies fundamental algorithms to evaluate similarity between TWO sources 
        (like cosine similarity, KMP...) and stores the results.
        """
        pass


    # Exemple d'utilisation
    source_path = "/home/daniel/Documents/INFO/OtherProjects/PlagiarismDetectionProject/Resources/Sample Data/SourceTexts/source-document01501.txt"
    tokenizer = Tokenizer(file=source_path)
    statistics = TextProcessingStatistics(base_content=tokenizer)
    statistics.display_statistics()
